# -*- coding: utf-8 -*-
"""2_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qptVVaSt53WLWyU05LI4YBmSn3ANQb3v
"""

!pip install transformers datasets

pip install tensorflow

from transformers import pipeline

classifier = pipeline("sentiment-analysis")

results = classifier(["We are very happy to show you the ðŸ¤— Transformers library.", "We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}")

#multilingual BERT model finetuned for sentiment analysis
#Use TFAutoModelForSequenceClassification and AutoTokenizer to load the pretrained model and its associated tokenizer
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained("nlptown/bert-base-multilingual-uncased-sentiment")
tokenizer = AutoTokenizer.from_pretrained("nlptown/bert-base-multilingual-uncased-sentiment")

classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
classifier("Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ðŸ¤— Transformers.")

#Under the hood, the AutoModelForSequenceClassification and AutoTokenizer classes work together to power the pipeline() you used above.
#An AutoClass is a shortcut that automatically retrieves the architecture of a pretrained model from its name or path.
#You only need to select the appropriate AutoClass for your task and itâ€™s associated preprocessing class.

#A tokenizer is responsible for preprocessing text into an array of numbers as inputs to a model.
#The most important thing to remember is you need to instantiate a tokenizer with the same model name
#to ensure youâ€™re using the same tokenization rules a model was pretrained with.

#Load a tokenizer with AutoTokenizer

from transformers import AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)

"""
The tokenizer returns a dictionary containing:
input_ids: numerical representations of your tokens.
attention_mask: indicates which tokens should be attended to
"""

encoding = tokenizer("We are very happy to show you the ðŸ¤— Transformers library.")
print(encoding)

"""
The most basic object in the ðŸ¤— Transformers library is the pipeline() function.
It connects a model with its necessary preprocessing and postprocessing steps
"""
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(
    ["I've been waiting for a HuggingFace course my whole life.", "I hate this so much!"]
)

"""
By default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English.
The model is downloaded and cached when you create the classifier object. If you rerun the command, the cached model will be used instead and there is no need to download the model again.

There are three main steps involved when you pass some text to a pipeline:

The text is preprocessed into a format the model can understand.
The preprocessed inputs are passed to the model.
The predictions of the model are post-processed, so you can make sense of them
"""

"""
classify texts that havenâ€™t been labelled.
For this use case, the zero-shot-classification pipeline allows you to specify
which labels to use for the classification, so you donâ€™t have to rely on the labels of the pretrained model
Youâ€™ve already seen how the model can classify a sentence as positive or negative using those two labels
but it can also classify the text using any other set of labels you like
This pipeline is called zero-shot because you donâ€™t need to fine-tune the model on your data to use it.
It can directly return probability scores for any list of labels you want!
"""


from transformers import pipeline

classifier = pipeline("zero-shot-classification")
classifier(
    "This is a course about the Transformers library",
    candidate_labels=["education", "politics", "business"],
)

"""Text generation
The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text.
Text generation involves randomness, so itâ€™s normal if you donâ€™t get the same results as shown below."""
from transformers import pipeline

generator = pipeline("text-generation")
generator("In this course, we will teach you how to")